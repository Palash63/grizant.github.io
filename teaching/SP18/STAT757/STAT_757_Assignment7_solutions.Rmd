---
title: "STAT 757 Assignment 7"
subtitle: "DUE 4/15/2018 11:59PM"
author: "AG Schissler"
date: "2/14/2018"
output: pdf_document
fig_caption: true
exclude: true
caption: true
references:
- id: sheather2009
  title: A modern approach to regression with R
  author:
  - family: Sheather
    given: Simon
  publisher: Springer Science \& Business Media
  type: book
  issued:
    year: 2009
---

## Instructions [20 points]

Modify this file to provide responses to the Ch.7 Exercises in @sheather2009. You can find some helpful code here: [http://www.stat.
tamu.edu/~sheather/book/docs/rcode/Chapter7.R](http://www.stat.tamu.edu/~sheather/book/docs/rcode/Chapter7.R). Also address the project milestones indicated below. Please email **both** your .Rmd (or roxygen .R) and one of the following either .HTML, .PDF, or .DOCX using the format SURNAME-FIRSTNAME-Assignment7.Rmd and SURNAME-FIRSTNAME-Assignment7.pdf.

## Exercise 7.5.3 [60 points] 

### Setup and fit model from 6.7.5

```{r, setup}
library(leaps)
library(car)
myDir <- "~/OneDrive - University of Nevada, Reno/Teaching/STAT_757/Sheather_data/Data/"
dat <- read.delim(file.path(myDir,"pgatour2006.csv"), sep = ",")
str(dat)
## subset to only the Y and seven predictors of interest
dat2 <- dat[,c("PrizeMoney", "DrivingAccuracy", "GIR", "PuttingAverage",
               "BirdieConversion", "SandSaves", "Scrambling", "PuttsPerRound")]
```

```{r, setup2}
m1 <- lm(log(PrizeMoney) ~ DrivingAccuracy + GIR +
             PuttingAverage + BirdieConversion + SandSaves +
             Scrambling + PuttsPerRound, data = dat2)
```

\newpage

### Part A

```{r, parta}
best_subsets_dat <- regsubsets(as.matrix(dat2[,-1]),log(dat2$PrizeMoney),
                               method = "exhaustive")
sum_best <- summary(best_subsets_dat)
sum_best
```

```{r, parta2, results = "asis"}
## The code below is to get AIC, AICc.
## store models in a list

all_mod <- vector(mode = "list", length = 7)
all_mod[[1]] <- lm(log(PrizeMoney)~GIR, data = dat2)
all_mod[[2]] <- lm(log(PrizeMoney)~GIR + PuttsPerRound, data = dat2)
all_mod[[3]] <- lm(log(PrizeMoney)~GIR + BirdieConversion + Scrambling, data = dat2)
all_mod[[4]] <- lm(log(PrizeMoney)~GIR + Scrambling + BirdieConversion + SandSaves,
                   data = dat2)
all_mod[[5]] <- lm(log(PrizeMoney)~GIR + PuttsPerRound + BirdieConversion +
                       SandSaves + Scrambling, data = dat2)
all_mod[[6]] <- lm(log(PrizeMoney)~GIR + PuttsPerRound + BirdieConversion +
                       SandSaves + Scrambling + DrivingAccuracy, data = dat2)
all_mod[[7]] <- lm(log(PrizeMoney)~., data = dat2)

## calculate AICc, AIC for each model
n <- nrow(dat2)
parta_mat <- do.call("rbind", lapply(all_mod, function(tmp_mod){
    ## number of parameters (p + 1 coefficients + sigma)
    npar <- length(tmp_mod$coefficients) + 1 
    ## Calculate AIC
    AIC <- extractAIC(tmp_mod,k=2)[2]
    ## Calculate AICc
    AICc <- (extractAIC(tmp_mod,k=2)+2*npar*(npar+1)/(n-npar-1))[2]
    ## Calculate BIC to check later
    BIC <- extractAIC(tmp_mod,k=log(n))[2]
    return(c(AIC = AIC, AICc = AICc, BIC = BIC))
}))

## BIC and R^2_adj are included in the summary
xtable(cbind(adjr2 = sum_best$adjr2, parta_mat))
```

The optimal model based on BIC is

$log(PrizeMoney) = \beta_0 + \beta_1 (GIR) + \beta_2 (BirdieConversion) + \beta_3 (Scrambling) + e$.

The other criterions agree that the best model is

$log(PrizeMoney) = \beta_0 + \beta_1 (GIR) + \beta_2 (BirdieConversion) + \beta_3 (Scrambling) + \beta_4 (PuttsPerRound) + \beta_5 (SandSaves) + e$.

### Part B

```{r, partb1}
## fit the full model to step backwards from
om7 <- lm( log(PrizeMoney) ~ ., data = dat2)
backAIC <- step(om7, direction="backward", data=dat2)
```

The final model by backwards selection in the last model described above:

$log(PrizeMoney) = \beta_0 + \beta_1 (GIR) + \beta_2 (PuttsPerRound) + \beta_3 (BirdieConversion) + \beta_4 (Scrambling) + \beta_5 (SandSaves) + e$.

Removing any fifth predictor in the model only increases AIC.

```{r, partb2}
## adjust the penalty term by log(n)
backBIC <- step(om7, direction="backward", data=dat2, k=log(n))
```

The final model by backwards selection in the last model described above:

$log(PrizeMoney) = \beta_0 + \beta_1 (GIR) + \beta_2 (BirdieConversion) + \beta_3 (Scrambling) + e$.

Removing any third predictor in the model only increases BIC.

### Part C

```{r, partc1}
mint <- lm( log(PrizeMoney) ~ 1, data=dat2 )
## AIC
forwardAIC <- step(mint,
                   scope=list(lower= ~ 1,
                              upper= ~ GIR + PuttsPerRound + BirdieConversion + SandSaves +
                                  Scrambling + DrivingAccuracy + PuttingAverage),
                              direction="forward", data=dat2)
```
The final model selected in the last model described above:

$log(PrizeMoney) = \beta_0 + \beta_1 (GIR) + \beta_2 (PuttsPerRound) + \beta_3 (BirdieConversion) + \beta_4 (Scrambling) + \beta_5 (SandSaves) + e$.

Adding any predictor as a sixth predictor in the model only increases AIC.

```{r, partc2}
## BIC
forwardBIC <- step(mint,
                   scope=list(lower= ~ 1,
                              upper= ~ GIR + PuttsPerRound + BirdieConversion + SandSaves +
                                  Scrambling + DrivingAccuracy + PuttingAverage),
                              direction="forward", data=dat2, k=log(n))
```
The final model selected in the last model described above:

$log(PrizeMoney) = \beta_0 + \beta_1 (GIR) + \beta_2 (PuttsPerRound) + \beta_3 (BirdieConversion) + \beta_4 (Scrambling) + e$.

Adding any predictor as a fifth predictor in the model only increases BIC.

### Part C

Both forward and backward selection are \emph{approximate} methods to find the "best" model. Neither method is guaranteed to find the optimal subset. On the other hand, an exhaustive search (best subsets) will find the optimal predictors given the data and a class of models. From an empirical/philosophical standpoint, the forward method starts with less information (only one predictor, then two predictors, etc) while backwards has all the information to consider before the first selection is made. This would seem to be a favorable situation for the backward selection process if computationally feasible. Then it does not surprise me that backwards and best subsets algorithms agree while forward selection does not (notice that the minimum BIC obtained by forward selection is greater than the backwards selection).


### Part D

I'd recommend a final model based on my perspective of the research goal. In Ch.6 (p. 224), the research question was stated as "what is the relative importance of each different aspect of the game on average prize money in professional golf"? As such, I prefer a fuller model to a more simpler model so that parameter estimates corresponding to the different aspects can be compared. With that in mind, I'd recommend the best subsets model with 5 predictors, namely

$log(PrizeMoney) = \beta_0 + \beta_1 (GIR) + \beta_2 (BirdieConversion) + \beta_3 (Scrambling) + \beta_4 (PuttsPerRound) + \beta_5 (SandSaves) + e$.

If the goal was to \emph{predict} prize money, then I would naively (or perform more sophicated predictive evaluation) go with the BIC model with 3 predictors. In general, it's best to use a model from best subsets as discussed above.

### Part E

```{r, parte}
final_model <- lm( log(PrizeMoney) ~ GIR + BirdieConversion + Scrambling +
                       PuttsPerRound + SandSaves, data=dat2 )
summary(final_model)
```

Both greens in regulation and birdie conversion are highly statistical significance (slopes are different from zero) and larger values correspond to more prize money. In light of the fact that the model is a result of data-driven variable selection, I'd say that the other three predictors are not different from zero at the 5\% significance level. That being said, there is a trend towards better scrambling/sand saves and fewer putts correspond to more prize money.

## Project milestones [20 points]

1. Conduct your data analysis plan.
 * Apply your model to fake data and ensure a proper fit.
 * Apply your model to real data.
 * Decide whether model is valid for the real data.
2. Refine your model as needed until you are satisfied with the fit.
 * Don't make decisions based on $p$-values or other inferential devices!
 * Only consider the fit and whether your model addresses your research hypothesis.

## References
